{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa97a12e",
   "metadata": {},
   "source": [
    "# -1) install la librairie \"findSpark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f423051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in /Users/romainjouin/opt/anaconda3/lib/python3.8/site-packages (1.4.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9531de63",
   "metadata": {},
   "source": [
    "# 0) configure 'findspark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4f48d2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ee894a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_home = \"/Users/romainjouin/Downloads/spark-3.1.2-bin-hadoop3.2\"\n",
    "findspark.init(spark_home=spark_home, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb05c50",
   "metadata": {},
   "source": [
    "# 1) configure spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e54ecccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark         import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql     import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d692370",
   "metadata": {},
   "source": [
    "## 1.1) création d'une configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "86f52440",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_url = \"spark://romainjouin-macbookpro1.roam.corp.google.com:7077\"\n",
    "nb_cpu     = 4\n",
    "App_name   = \"Nom de l'application\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcdc116",
   "metadata": {},
   "source": [
    "[Propriétés de Spark](https://spark.apache.org/docs/latest/configuration.html#available-properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c5648111",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf = conf.setMaster(master_url)\n",
    "conf = conf.set(\"spark.cores.max\", nb_cpu)\n",
    "conf = conf.setAppName(App_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba8faee",
   "metadata": {},
   "source": [
    "## 1.2) creation d'un spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7c92dc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "try    : sc.stop()\n",
    "except : pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f9bf11fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f92fd065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://romainjouin-macbookpro1.roam.corp.google.com:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Nom de l'application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://romainjouin-macbookpro1.roam.corp.google.com:7077 appName=Nom de l'application>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355e94f0",
   "metadata": {},
   "source": [
    "# 2) lecture d'un fichier parquet file (slide 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a6088fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_dir            = \"./../Downloads/spark-3.1.2-bin-hadoop3.2\"\n",
    "path_to_parquet_file = f\"{spark_dir}/examples/src/main/resources/users.parquet\"\n",
    "path_to_readme       = f\"{spark_dir}/README.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2b559671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89248bdd",
   "metadata": {},
   "source": [
    "# Coder une requête sql en map reduce\n",
    "SELECT   mot, count(mot)  \n",
    "FROM     fichier  \n",
    "WHERE    len(mot)>3  \n",
    "GROUP BY mot  \n",
    "LIMIT    10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0182da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import functions as F\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "82282a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('unified', 1),\n",
       " ('analytics', 1),\n",
       " ('engine', 2),\n",
       " ('large', 1),\n",
       " ('provides', 1),\n",
       " ('high', 1),\n",
       " ('scala', 4),\n",
       " ('java', 1),\n",
       " ('python', 4),\n",
       " ('optimized', 1)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fichier        = sc.textFile(path_to_readme)\n",
    "mots           = fichier.flatMap(lambda ligne: re.split(\"\\W+\", ligne.lower().strip()))\n",
    "mots_de_3_car  = mots.filter(lambda mot : len(mot)>3)\n",
    "k_v            = mots_de_3_car.map(lambda mot: (mot, 1))\n",
    "denombrement   = k_v.reduceByKey(add)\n",
    "dix_examples   = denombrement.take(10)\n",
    "dix_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852d1189",
   "metadata": {},
   "source": [
    "# 3) création d'une spark session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a10712e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_Session = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bc5f17d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.21:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://romainjouin-macbookpro1.roam.corp.google.com:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Nom de l'application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8f7950b9a0>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd27217e",
   "metadata": {},
   "source": [
    "## 3.1) utiliser la sparkSession pour lire des CSV avec headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "57d0598b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.readwriter.DataFrameReader"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = spark_Session.read.option(\"header\", \"true\")\n",
    "type(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bebd831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_csv = \"./2021_esilv_spark/cycliste_debug.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2b8a4da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reader.csv(path_to_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b825562b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(cycliste='cycliste_azey2', n_message='0442', time=' 1486038900.39', message='velo rendu sur la station azgb6 ( a la maison = False) (elem de station_travail)')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649d41dc",
   "metadata": {},
   "source": [
    "## 3.2) possible de convertir la DataFrame en RDD \n",
    "\n",
    "Les dataframes sont bassées sur des RDD\n",
    "En spark 1 les RDD étaient les objets de bases, maintenant on promeut plutôt les df => qui ont un schéma est sont plus optimisées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c163ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cd65c443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cycliste='cycliste_azey2', n_message='0442', time=' 1486038900.39', message='velo rendu sur la station azgb6 ( a la maison = False) (elem de station_travail)'),\n",
       " Row(cycliste='cycliste_azey2', n_message='0441', time=' 1486038900.39', message='impossible de rendre sur '),\n",
       " Row(cycliste='cycliste_azey2', n_message='0440', time=' 1486038900.38', message='self.a_la_maison = False')]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a22406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d13133f0",
   "metadata": {},
   "source": [
    "# 4) Sql Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1042e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SQLContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36994014",
   "metadata": {},
   "source": [
    "## 4.1) création du sql context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "571ec3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35386f31",
   "metadata": {},
   "source": [
    "## 4.2) lecture d'un fichier => création dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5aff8046",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.load(path_to_parquet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5314f1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alyssa', favorite_color=None, favorite_numbers=[3, 9, 15, 20]),\n",
       " Row(name='Ben', favorite_color='red', favorite_numbers=[])]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5bf5b6",
   "metadata": {},
   "source": [
    "## 4.3) enregistrement d'une table temporaire en mémoire acceptant ensuite des requêtes SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "67b4c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nom_de_la_table = \"XY\"\n",
    "sqlContext.registerDataFrameAsTable(df, nom_de_la_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4ec0b2d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Ben', count(1)=1), Row(name='Alyssa', count(1)=1)]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requete_sql = f\"\"\"\n",
    "                    Select name, count(*)\n",
    "                    FROM {nom_de_la_table}\n",
    "                    group by name\n",
    "\"\"\"\n",
    "sql_result = sqlContext.sql(requete_sql)\n",
    "sql_result.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44b65586",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_to_parquet_file = \"./../Downloads/spark-3.1.2-bin-hadoop3.2/examples/src/main/resources/people.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b448fec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name;age;job\n",
      "Jorge;30;Developer\n",
      "Bob;32;Developer\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_parquet_file) as f:\n",
    "    for i in range(100):\n",
    "        print(f.readline(), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "221dd5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = sqlcontext.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b098819",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = reader.option(\"delimiter\", \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee381d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(nom='name', age=None, job='job'),\n",
       " Row(nom='Jorge', age=30.0, job='Developer'),\n",
       " Row(nom='Bob', age=32.0, job='Developer')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = reader.load(path_to_parquet_file, format=\"csv\", schema=\"nom STRING,age FLOAT,job STRING\", )\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb01d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
